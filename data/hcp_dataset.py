"""FOD-Net
Fiber orientation distribution super resolution
Licensed under the CC BY-NC-SA 4.0 License (see LICENSE for details)
Written by Rui Zeng @ The University of Sydney (r.zeng@outlook.com / rui.zeng@sydney.edu.au)
"""

import os.path
from data.base_dataset import BaseDataset
import numpy as np
import nibabel as nib

def nullable_string(val):
    if not val:
        return None
    return val

def nullable_int(val):
    if not val:
        return None
    return int(val)

def flip_axis_to_match_HCP_space(data, affine):
    """
    Checks if affine of the image has the same signs on the diagonal as HCP space. If this is not the case it will
    invert the sign of the affine (not returned here) and invert the axis accordingly.
    If displayed in an medical image viewer the image will look the same, but the order by the data in the image
    array will be changed.

    This function is inspired by the function with the same name in TractSeg
    """
    newAffine = affine.copy()  # could be returned if needed
    flipped_axis = []

    if affine[0, 0] > 0:
        flipped_axis.append("x")
        data = data[::-1, :, :]
        newAffine[0, 0] = newAffine[0, 0] * -1
        newAffine[0, 3] = newAffine[0, 3] * -1

    if affine[1, 1] < 0:
        flipped_axis.append("y")
        data = data[:, ::-1, :]
        newAffine[1, 1] = newAffine[1, 1] * -1
        newAffine[1, 3] = newAffine[1, 3] * -1

    if affine[2, 2] < 0:
        flipped_axis.append("z")
        data = data[:, :, ::-1]
        newAffine[2, 2] = newAffine[2, 2] * -1
        newAffine[2, 3] = newAffine[2, 3] * -1

    return data, newAffine, flipped_axis

class HCPDataset(BaseDataset):
    """
    This dataset class can load the processed HCP dataset for angular super resolution in terms of fiber orientation
    distribution function computed by constrained spherical deconvolution.

    The processed HCP dataset is supposed to have FOD images generated by low angular resolution (typical single shell single
     tissue 32 gradient directions) DWI and original DWI (multi shell 1000, 2000, 3000, totally 288 gradient directions)

     This class is a heavy memory version, which loads every samples into the memory for further use.
    """

    def __init__(self, opt):
        """Initialize this dataset class.

        Parameters:
            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions
        """
        super(HCPDataset, self).__init__(opt)
        self.size_3d_patch = opt.size_3d_patch
        self.margin = int(self.opt.size_3d_patch/2)
        self.dataset_num_samples_per_data = int((120 / self.size_3d_patch / (1 - opt.dataset_samples_overlapping_rate)) ** 3 )
        self._fod_ids = []
        self.fod_info = []
        self.load_hcp(dataset_path=opt.dataroot)
        self.prepare()

    def __getitem__(self, index):
        """Return a data point and its metadata information.

        Parameters:
            index (int)      -- a random integer for data indexing

        Returns a dictionary that contains training data and corresponding golden standard
            fodlar (tensor)  -- the spherical harmonics coefficients of the fODF obtained from LAR DWI
            fodgt (tensor)   -- the spherical harmonics coefficients of the fODF obtained from original HCP DWI
        """
        fod_sample = self.fod_info[index % self.num_fod]

        fodlr = fod_sample['fodlr']
        fodgt = fod_sample['fodgt']
        final_index = fod_sample['index_mask']
        index_length = fod_sample['index_length']

        idx = np.random.randint(index_length)
        x = final_index[0, idx]
        y = final_index[1, idx]
        z = final_index[2, idx]

        x_start = x - self.margin
        x_end = x_start + self.opt.size_3d_patch
        y_start = y - self.margin
        y_end = y_start + self.opt.size_3d_patch
        z_start = z - self.margin
        z_end = z_start + self.opt.size_3d_patch
        fodlr_3D_patches = fodlr[x_start:x_end, y_start:y_end, z_start:z_end, :]
        fodgt_3D_patches = fodgt[x_start:x_end, y_start:y_end, z_start:z_end, :]

        # Extract 3D patches for training
        fodgt_centre = fodgt_3D_patches[4, 4, 4, :]

        data_dict = {'fodlr': fodlr_3D_patches.transpose(3, 0, 1, 2),
                     'fodgt': fodgt_centre}

        return data_dict


    def load_hcp(self, dataset_path):
        """Load the processed hcp dataset
        dataset_dir: The root directory of the processed hcp dataset.
        """
        subject_ids = os.listdir(dataset_path)

        # Add fod samples
        for i, subject_id in enumerate(subject_ids):
            print('Loading {0}'.format(subject_id))
            self.add_fod_sample(fod_id=i,
                                fodlr_path=os.path.join(dataset_path, subject_id, "ss3t_csd", "WM_FODs_normalised.nii.gz"),
                                fodgt_path=os.path.join(dataset_path, subject_id, "msmt_csd", "WM_FODs_normalised.nii.gz"),
                                fsl_5ttgen_mask_path=os.path.join(dataset_path, subject_id, "fsl_5ttgen.nii.gz"),
                                subject_id=subject_id)

    def add_fod_sample(self, fod_id, fodlr_path, fodgt_path, fsl_5ttgen_mask_path, subject_id=None):
        fodlr = nib.load(fodlr_path)
        fodgt = nib.load(fodgt_path)
        fsl5ttgen_mask = nib.load(fsl_5ttgen_mask_path)

        fodlr, fixed_fodlr_affine, flipped_axis_fodlr = flip_axis_to_match_HCP_space(fodlr.get_data(), fodlr.affine)
        fodgt, fixed_fodgt_affine, flipped_axis_fodgt = flip_axis_to_match_HCP_space(fodgt.get_data(), fodgt.affine)
        fsl5ttgen_mask, fixed_fsl5ttgen_mask_affine, flipped_axis_fsl5ttgen_mask = flip_axis_to_match_HCP_space(fsl5ttgen_mask.get_data(), fsl5ttgen_mask.affine)
        fsl5ttgen_mask = np.clip(fsl5ttgen_mask, a_min=0.0, a_max=1.0)
        
        cutted_x, cutted_y, cutted_z, _ = fsl5ttgen_mask.shape
        # Including white matter, cortical grey matter, and subcortical grey matter tissues can improve the performance
        index_mask = np.where(fsl5ttgen_mask[:, :, :, :3].any(axis=-1)) 
        index_mask = np.asarray(index_mask)
        x = index_mask[0, :]
        y = index_mask[1, :]
        z = index_mask[2, :]
        x_mask = np.logical_and(x >= self.margin, x < (cutted_x - self.margin))
        y_mask = np.logical_and(y >= self.margin, y < (cutted_y - self.margin))
        z_mask = np.logical_and(z >= self.margin, z < (cutted_z - self.margin))
        coord_mask = np.logical_and.reduce([x_mask, y_mask, z_mask])
        final_index = index_mask[:, coord_mask]
        index_length = len(final_index[0])

        fod_info = {
            "id": fod_id,
            'fodlr': fodlr,
            'fodgt': fodgt,
            'subject_id': subject_id,
            'index_mask': final_index,
            'index_length': index_length,
        }
        self.fod_info.append(fod_info)

    def prepare(self):
        """Prepares the Dataset class for use.
        1. Compute the number of the fod lr/gt pairs
        2. Build the index for each fod sample
        """
        self.num_fod = len(self.fod_info)
        self._fod_ids = np.arange(self.num_fod)
        self.epoch_step = self.dataset_num_samples_per_data * self.num_fod

    @property
    def fod_ids(self):
        return self._fod_ids

    def __len__(self):
        """Return the total number of fod samples in the dataset.
        """
        return self.epoch_step

    #
    # Utils functions
    #

    @staticmethod
    def modify_commandline_options(parser, is_train=True):
        """Add new dataset-specific options, and rewrite default values for existing options.

        Parameters:
            parser          -- original option parser
            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.

        Returns:
            the modified parser.
        """
        parser.add_argument('--size_3d_patch', type=int, default=9,
                            help='the size of the 3D patches')
        parser.add_argument('--dataset_samples_overlapping_rate', type=float, default=0.3,
                            help='How many samples for each subject we want to randomly sample in one epoch')
        if is_train:
            parser.add_argument('--shuffle', type=bool, default=True, help='Shuffle the dataset during training')
        else:
            parser.add_argument('--shuffle', type=bool, default=False,
                                help='Do not shuffle the dataset during eval/val')
        return parser